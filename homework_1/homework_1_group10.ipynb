{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 1\n",
        " \n",
        "## Group 10\n",
        "\n",
        "## 1. Understanding MDPs\n",
        "\n",
        "### 1.1 Chess\n",
        "\n",
        "- States S: All conceivable legal game states.\n",
        "- Actions A: All legal moves.\n",
        "- Probabilistic state dynamics: Deterministic for the game of chess. We can be certain that if we take an action we always end up in the state we want.\n",
        "- Reward dynamics: The reward you get changes based on the value of the piece you capture. \n",
        "- Initial state $p_0$: All pieces positioned in their beginning state. No action has been taken.\n",
        "- Policy π: The policy describes what legal move A to take for each game state S\n",
        "\n",
        "\\\\\n",
        "\n",
        "### 1.2 LunarLander\n",
        "\n",
        "- States S: 8-dimensional vector with x and y being the coordinates of the lander, its linear velocity in x and y, its angle, its angular velocity, two booleans that represent whether each leg is connected to the ground.\n",
        "- Actions A: Do nothing, fire left orientation engine, fire right orientation engine, fire the main engine.\n",
        "- Probabilistic state dynamics: Dependent on the physics simulation of the environment.\n",
        "- Reward dynamics: Crashing (-100), Coming to rest (+100), Each leg with ground contact (+10), firing main engine (-0.3), Firing side engine (-0.03), Solved (+200).\n",
        "- Initial state: The lander is located at the top center.\n",
        "- Policy π: The policy describes what move A to take for each game state S.\n",
        "\n",
        "\\\\\n",
        "\n",
        "### 1.3 Model Based RL: Accessing Environment Dynamics\n",
        "\n",
        "Policy evaluation evaluates the current policy and computes the Value function $V(s) = \\pi(a|s) \\sum_s',r [p(s', r|s,a)(r+γV_π(s'))]]$\n",
        "\n",
        "Polivy Iteration updates the  policy based on the result of the policy evaluation, by updating the optimal action to take for each state s, based on its value.\n",
        "\n",
        "\\\\\n",
        "\n",
        "The environment dynamics is a combination of reward function and state transition dynamics. Based on the action a that is taken in the state s, the environment dynamics define the reward and the state s' we end up in. For instance, an agent would gain a certain reward by collecting the points and reaching to the end of the respective level in the game of Super Mario. Also taking action a leads to state s' based on the velocity and the current position of mario. Another example would be that of the robotic arm trying to manipulate or move a certain object. The agent would aquire some rewards for sucessfully moving the object and the state transition dynamics would be described by real world physics. \n",
        "\n",
        "\\\\\n",
        "\n",
        "The true environment dynamics are not always known. When considering the Mario example again. Even if we have all the visual information of the game and we know exactly how the character moves (in terms of physics) our moves could still be obstructed by hidden blocks. Also the reward function is defined by us in an arbitrary fashion. Even if we are not certain about the true underlying environment dynamics we can still try and approximate reality and use the approximation for our training."
      ],
      "metadata": {
        "id": "VoLc0JWRPiSj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Implementing a GridWorld\n",
        "\n",
        "### 2.1 Look up some examples\n",
        "\n",
        "1. Classic Grid World\n",
        "https://notebook.community/spro/practical-pytorch/reinforce-gridworld/reinforce-gridworld\n",
        "\n",
        "2. Windy Grid World\n",
        "https://wocken-datenliebe.com/2019/06/21/windy-gridworld-rl/\n",
        "\n",
        "3. Multi-Agent Grid World\n",
        "https://github.com/opocaj92/GridWorldEnvs\n",
        "\n",
        "### 2.2 Implementing the MDP\n",
        "\n",
        "- The MDP uses a stochastic policy \n",
        "- The two complexities included are a trap at tile 4,4 and a blocked tile at 2,2"
      ],
      "metadata": {
        "id": "vQKHUePdtPUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random as rd"
      ],
      "metadata": {
        "id": "Iycz1f2tzjOR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "fJwfoWOFPdkk"
      },
      "outputs": [],
      "source": [
        "LEFT = 0\n",
        "TOP = 1\n",
        "RIGHT = 2\n",
        "DOWN = 3\n",
        "\n",
        "class GridWorld():\n",
        "    def __init__(self, size=(5,6)):\n",
        "        self.size = size\n",
        "        self.board = np.zeros(size)\n",
        "        self.board[4,3] = 1\n",
        "        self.board[4,4] = -1\n",
        "        self.state = (0,0)\n",
        "\n",
        "    def get_size(self):\n",
        "        return self.size\n",
        "\n",
        "    def get_state(self):\n",
        "        return self.state\n",
        "\n",
        "    def get_reward(self):\n",
        "        return self.board[self.state]\n",
        "\n",
        "    def is_legal(self, next_state):\n",
        "        if ((next_state[0] >= 0) and (next_state[0] < len(self.board))):\n",
        "            if ((next_state[1] >= 0) and (next_state[1] < len(self.board[0]))):\n",
        "                if(next_state != (2,2)):\n",
        "                    return True \n",
        "                else: \n",
        "                    return False\n",
        "    \n",
        "    def is_goal(self):\n",
        "        if (self.board[self.state] == -1) or (self.board[self.state] == 1):\n",
        "            return True\n",
        "        else: \n",
        "            return False\n",
        "\n",
        "    def move(self, action): \n",
        "        # origin at top left\n",
        "        # TOP\n",
        "        if action == TOP: \n",
        "            next_state = (self.state[0] - 1, self.state[1])\n",
        "        # RIGHT\n",
        "        if action == RIGHT:\n",
        "            next_state = (self.state[0], self.state[1] + 1)\n",
        "        # DOWN\n",
        "        if action == DOWN:\n",
        "            next_state = (self.state[0] + 1, self.state[1])\n",
        "        # LEFT\n",
        "        if action == LEFT:\n",
        "            next_state = (self.state[0], self.state[1] -1)\n",
        "        # check if move is legal\n",
        "        if self.is_legal(next_state):\n",
        "            self.state = next_state\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = (0,0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Implementing a policy \n",
        "\n",
        "### 3.1 Implementing the basic agent"
      ],
      "metadata": {
        "id": "uk1KMemtimcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent():\n",
        "    def __init__(self):\n",
        "        self.grid_world = GridWorld()\n",
        "        self.actions = [TOP, RIGHT, DOWN, LEFT]\n",
        "    \n",
        "    def get_action(self):\n",
        "        r = rd.random()\n",
        "        if(r < 0.8):\n",
        "            return rd.randint(2,3)\n",
        "        if(r >= 0.8):\n",
        "            return rd.randint(0,1)\n",
        "\n",
        "    def walk(self):\n",
        "        episode = []\n",
        "        self.grid_world.reset()\n",
        "        while(not self.grid_world.is_goal()):\n",
        "            state = self.grid_world.get_state()\n",
        "            action = self.get_action()\n",
        "            self.grid_world.move(action) \n",
        "            reward = self.grid_world.get_reward()\n",
        "            episode.append((state, action, reward))\n",
        "        return episode\n",
        "        "
      ],
      "metadata": {
        "id": "CpKGZk6Jijvd"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Evaluate the policy"
      ],
      "metadata": {
        "id": "_47G5rVzit_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent()\n",
        "gamma = 0.9\n",
        "\n",
        "V = {}\n",
        "N = {}\n",
        "for i in range(agent.grid_world.get_size()[0]):\n",
        "    for j in range(agent.grid_world.get_size()[1]):\n",
        "        s = (i, j)\n",
        "        V[s] = 0\n",
        "        N[s] = 0\n",
        "\n",
        "for ep in range(1000):\n",
        "    episode = agent.walk()\n",
        "    G = 0\n",
        "    visited = []\n",
        "    for i, (s, a, r) in enumerate(episode[::-1]):\n",
        "        if (s == (4,3)):\n",
        "            print(\"h\")\n",
        "        G = gamma * G + r\n",
        "        if s not in visited:\n",
        "            visited.append(s)\n",
        "            N[s] += 1\n",
        "            V[s] += (G - V[s]) / N[s]\n",
        "\n",
        "for i in range(agent.grid_world.get_size()[0]):\n",
        "    for j in range(agent.grid_world.get_size()[1]):\n",
        "        s = (i,j)\n",
        "        print(\"V({}) = {}\".format(s, V[s]))"
      ],
      "metadata": {
        "id": "EkBlaYmZ3ERJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02dfcee8-0913-4140-bf80-819bd5c9ed1e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "V((0, 0)) = 0.08165910264121884\n",
            "V((0, 1)) = 0.01018289956785814\n",
            "V((0, 2)) = -0.07141164490926206\n",
            "V((0, 3)) = -0.1287400542778063\n",
            "V((0, 4)) = -0.18070942640115667\n",
            "V((0, 5)) = -0.20953826176838725\n",
            "V((1, 0)) = 0.1769897251345133\n",
            "V((1, 1)) = 0.10483687358627423\n",
            "V((1, 2)) = -0.05218940480790797\n",
            "V((1, 3)) = -0.10580964523297216\n",
            "V((1, 4)) = -0.24423254608270828\n",
            "V((1, 5)) = -0.28080171398178194\n",
            "V((2, 0)) = 0.2896261493663063\n",
            "V((2, 1)) = 0.33608438178639605\n",
            "V((2, 2)) = 0\n",
            "V((2, 3)) = -0.04402424200270676\n",
            "V((2, 4)) = -0.3620308220617057\n",
            "V((2, 5)) = -0.38597031624824113\n",
            "V((3, 0)) = 0.3876444435072291\n",
            "V((3, 1)) = 0.4808409389353765\n",
            "V((3, 2)) = 0.47959218539631326\n",
            "V((3, 3)) = 0.24028481278011565\n",
            "V((3, 4)) = -0.6211441501602045\n",
            "V((3, 5)) = -0.6337651924860249\n",
            "V((4, 0)) = 0.5211567736007382\n",
            "V((4, 1)) = 0.6923447274594741\n",
            "V((4, 2)) = 0.9071323119998644\n",
            "V((4, 3)) = 0\n",
            "V((4, 4)) = 0\n",
            "V((4, 5)) = -0.9341415098553923\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8xLXmjvrjTp3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}